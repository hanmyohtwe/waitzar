#summary Can we modernize the model format without sacrificing efficiency?
#labels Phase-Design

= Introduction =

The old model format was written back when I knew little about C++. (It was implemented using a massive double array of pointers, to much shuddering.) Later, I removed the pointers, but kept things like word IDs, since I was worried about using maps for jump lists, etc. Now, I think we can store data in memory in a way that is even simpler; for example, using a map of words-to-prefixes to avoid a whole bunch of array entries.

Throughout this process, the "binary-ish" style of the model hasn't been updated. I'd like to use the JSON parser (capitalizing on the full power of JSON, including numbers and arrays) to read model files, in the interest of robustness, code reuse, etc. So, this page will attempt to review both the new storage format (on disk) and its representation in memory.


= Old Storage Format =

The old storage format went something like this:
  * Several sections, each labeled with a "#ID: XXXX", where "ID" was "DEFINE", "MAP", or "SEGMENT", and "XXXX" was the number of entries (lines, usually, except in "DEFINE") in that section. Extra comments caused parse errors. 
  * DEFINE contained several lines, each of the form "XXX[AA,BB-CC-DD,...,ZZ]", where "XXX" was the ID of the first word on that line (used only when manually debugging the file), and each element in the array was a word in the dictionary, separated by commas. If a word had multiple letters, these were separated by dashes. Each line contained 50 definitions. Oh, and "AA" actually meant "\u10AA", since Myanmar was assumed (and Unicode 5.2 hadn't been invented yet).
  * MAP looked something like this: "{L:DDD,L:DDD,...}". Here, "L" was a single character representing a "move" one could make from that node and "DDD" was the ID (non-hex!) of the next map node to jump to. If the letter was "~", then the ID was actually the ID of the PREFIX entry to jump to.
  * PREFIX lines were even more convoluted: "{AAA:BBB,CCC:DDD}[XXX,YYY,ZZZ]". The first bracketed list was a hash lookup; if a word with ID "AAA" was in the current n-gram then prefix id "BBB" was jumped to. This process continued until no more trigrams existed/matched, at which point the "XXX", "YYY", id words represented definitions matched at that prefix id. (All numbers are non-hex). However! when adding words, one was required to start at the _last_ matching prefix and add all words in that prefix and _each_ prefix preceding that match until all words had been added. This allowed for trigrams to re-order matched words. 

As you can see, the format was haphazard and required a good deal of explaining. There's nothing wrong with it _per se_, and it had the benefit of being usable in any language with simple arrays --no need for hash tables, pointers, or even array resizing. That said, I think a more portable solution is in order, especially since this format can be abused (e.g., adding a definition that is ONLY in the list if the appropriate trigram matches, which is not what WZ intended). 

...by the way, the first 10 definitions were _required_ to be the numbers 1 through 0 (in Burmese, of course). Ok, ok, enough of the old format.


= New Format: Definitions =

Assuming that we use JSON (which is all but certain, since the format is very flexible _and_ we have a small parser for it), the definitions are easy:

{{{
#Note that definitions (words) are still identified by ID
{ 
  "words" : ["၁", "၂", ..., "ကို"]
}
}}}

There are no restrictions on what words may be stored in a dictionary.


= New Format: Node Tree =

Two facts are clear about the Wait Zar romanization. First, there was never any slowdown due to our decision to store node lookups in an array instead of a map. Given the average length of a syllable and the number of available letters in the English alphabet, I'd expect this to be true for any language. (The JSON Spirit author notes that a typical PC begins to favor maps after 500 elements, which requires syllables 20 letters long where _each_ of the 24 English letters is equally likely at any point in the word. I'll happily reconsider if you show me a language that fits this criteria and want to implement in Wait Zar.) Second, nodes only ever had a single parent, so the integer index for each node could easily be replaced by a pointer of some kind without increasing the memory used at run-time; i.e., we could just build a map of maps of maps... if we wanted to.

There's one problem with this: currently, typing "g" presumes "aung", which only applies to Burmese. I'd like to lift this feature from the code into the model file. We'll get to that later.

Let's first consider the most direct conversion from our old format:

{{{
{
  "lookup" : [
    [{"a":10}, {"b",20}],
    [{"k":11}, {"~",2}],
    ...
  ]
}
}}}

This approach actually seems more archaic, due to all the new curly braces. We can clean it up a bit by grouping the lookup letters into a string and indexing by that into an array:

{{{
{
  "lookup" : [
    {"abc" : [12,23,34]},
    {"ka~" : [100,10,15]},
    ...
  ]
}
}}}

...but that doesn't improve much. We could try separating out the "keys" into one array and the "values" into another:

{{{
{
  "lookup" : [
    ["abc", "ka~", ...,],
    [[12,23,34], [100,10,15], ...,],
    ...
  ]
}
}}}

This is probably the best solution that uses IDs. If we choose to nest the maps recursively, we end up with:

{{{
{
  "lookup" : {
    "a" : {
        "b" : {...},
        "c" : {...},
        "~" : 100
    } ,
    "k" : {
        "o" : {...},
        "~" : 20
    }
  }
}
}}}

This is actually pretty easy on the eyes, and since models are always generated by script it wouldn't be that hard to build. We could even fold in the main prefix entry by having "~" refer to an array of words instead of a prefix offset.

{{{
{
  "lookup" : {
    "a" : {
        "b" : {...},
        "c" : {...},
        "~" : [100,200,110]
    } ,
    "k" : {
        "o" : {...},
        "~" : [12]
    }
  }
}
}}}

Each node with a "~" will ALWAYS have at least one entry, so we aren't wasting any space. Note that we can't replace word IDs with actual words because (#1) prefix entries index words multiple times and (#2) a word can appear under different romanizations and should still be considered the "same" word.





